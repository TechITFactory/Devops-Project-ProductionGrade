# Sprint 2, Story 4.1: Provision EKS Cluster

## Story Summary
Deploy the EKS cluster using the already-complete EKS module.

**What We're Deploying:**
- EKS Control Plane with OIDC
- Managed Node Group (2x t3.medium)
- IAM Roles for cluster and nodes
- EBS CSI Driver for PersistentVolumes
- OIDC Provider for IRSA
- Cluster Autoscaler IRSA role
- ALB Controller IRSA role

> **â±ï¸ Time:** EKS cluster creation takes ~15-20 minutes.
> **ðŸ’° Cost:** ~$147/month (control plane $72 + nodes $60 + EBS $10 + logs $5)

---

## Prerequisites
- S3.1 completed (VPC deployed)
- VPC outputs available (vpc_id, private_subnet_ids)

---

## Part 0: Verify Prerequisites

### Step 0.1: Verify VPC is Deployed
```bash
cd ~/Desktop/Devops-Project/techitfactory-infra/environments/dev

# Check VPC outputs
terraform output vpc_id
terraform output private_subnet_ids
# If you get errors, complete S3.1 first!
```

---

## Part 1: Understanding EKS Architecture

### The Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           AWS Account                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚               EKS Control Plane (AWS Managed)                 â”‚  â”‚
â”‚  â”‚     API Server | etcd | Scheduler | Controller Manager        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                              â”‚ HTTPS                                â”‚
â”‚                              â”‚ OIDC Provider (for IRSA)             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              Managed Node Group (Private Subnets)              â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚  â”‚
â”‚  â”‚  â”‚t3.mediumâ”‚  â”‚t3.mediumâ”‚  â† Auto-scaled by Cluster Autoscalerâ”‚  â”‚
â”‚  â”‚  â”‚ Node 1  â”‚  â”‚ Node 2  â”‚                                     â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                     â”‚
â”‚  Add-ons: EBS CSI | CoreDNS | kube-proxy | VPC CNI                 â”‚
â”‚  IRSA Roles: Cluster Autoscaler | ALB Controller | EBS CSI         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Components:**
- **Control Plane**: AWS managed, $72/month, no maintenance needed
- **Node Group**: EC2 instances running your pods
- **OIDC Provider**: Enables pods to assume IAM roles (IRSA)
- **Add-ons**: AWS-managed Kubernetes components

---

## Part 2: Review EKS Module Code

> **âœ… The EKS module code is already complete in the repo!**
> Let's understand what each resource does.

### Step 2.1: Navigate to EKS Module
```bash
cd ~/Desktop/Devops-Project/techitfactory-infra/modules/eks
ls -la
```

### Step 2.2: Understanding main.tf

#### Cluster IAM Role
```hcl
resource "aws_iam_role" "cluster" {
  name = "${local.cluster_name}-cluster-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Action = "sts:AssumeRole"
      Effect = "Allow"
      Principal = {
        Service = "eks.amazonaws.com"  # ðŸ‘ˆ EKS service can assume this role
      }
    }]
  })
}

resource "aws_iam_role_policy_attachment" "cluster_policy" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSClusterPolicy"
  role       = aws_iam_role.cluster.name
}
```

**Why do we need this?**
- EKS control plane needs permissions to manage AWS resources
- AWS managed policies contain all required permissions
- Never create inline policies - use AWS managed ones

#### EKS Cluster
```hcl
resource "aws_eks_cluster" "main" {
  name     = local.cluster_name
  role_arn = aws_iam_role.cluster.arn
  version  = var.cluster_version           # 1.28

  vpc_config {
    subnet_ids              = var.subnet_ids        # Private subnets
    endpoint_public_access  = true                  # kubectl from internet
    endpoint_private_access = true                  # Nodes access API internally
    security_group_ids      = [aws_security_group.cluster.id]
  }

  # Enable control plane logging
  enabled_cluster_log_types = ["api", "audit", "authenticator", "controllerManager", "scheduler"]
}
```

**Key decisions:**
- `endpoint_public_access = true`: Allows kubectl from your laptop
- `endpoint_private_access = true`: Nodes communicate internally
- **Logging**: Audit logs are critical for security compliance

#### OIDC Provider (for IRSA)
```hcl
data "tls_certificate" "cluster" {
  url = aws_eks_cluster.main.identity[0].oidc[0].issuer
}

resource "aws_iam_openid_connect_provider" "cluster" {
  url             = aws_eks_cluster.main.identity[0].oidc[0].issuer
  client_id_list  = ["sts.amazonaws.com"]
  thumbprint_list = [data.tls_certificate.cluster.certificates[0].sha1_fingerprint]
}
```

**What is IRSA?**
- **I**AM **R**oles for **S**ervice **A**ccounts
- Allows pods to assume IAM roles without access keys
- Much more secure than node-level IAM roles
- Required for: ALB Controller, Cluster Autoscaler, EBS CSI

#### Node IAM Role
```hcl
resource "aws_iam_role" "node" {
  name = "${local.cluster_name}-node-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Action = "sts:AssumeRole"
      Effect = "Allow"
      Principal = {
        Service = "ec2.amazonaws.com"  # ðŸ‘ˆ EC2 instances assume this
      }
    }]
  })
}

# Required AWS managed policies
resource "aws_iam_role_policy_attachment" "node_worker" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"
  role       = aws_iam_role.node.name
}

resource "aws_iam_role_policy_attachment" "node_cni" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"
  role       = aws_iam_role.node.name
}

resource "aws_iam_role_policy_attachment" "node_ecr" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
  role       = aws_iam_role.node.name
}

resource "aws_iam_role_policy_attachment" "node_ssm" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore"
  role       = aws_iam_role.node.name
}
```

**Why these policies?**
- `EKSWorkerNodePolicy`: Register with EKS control plane
- `EKS_CNI_Policy`: VPC networking for pods
- `EC2ContainerRegistryReadOnly`: Pull images from ECR
- `SSMManagedInstanceCore`: SSH into nodes via Session Manager (no SSH keys!)

#### Managed Node Group
```hcl
resource "aws_eks_node_group" "main" {
  cluster_name    = aws_eks_cluster.main.name
  node_group_name = "${local.cluster_name}-nodes"
  node_role_arn   = aws_iam_role.node.arn
  subnet_ids      = var.subnet_ids           # ðŸ‘ˆ Private subnets only

  instance_types = var.node_instance_types   # ["t3.medium"]
  capacity_type  = var.node_capacity_type    # ON_DEMAND or SPOT
  disk_size      = var.node_disk_size        # 50 GB

  scaling_config {
    desired_size = var.node_desired_size     # 2
    max_size     = var.node_max_size         # 4
    min_size     = var.node_min_size         # 1
  }

  tags = {
    "k8s.io/cluster-autoscaler/enabled"                = "true"
    "k8s.io/cluster-autoscaler/${local.cluster_name}" = "owned"
  }
}
```

**Important tags for Cluster Autoscaler:**
- `k8s.io/cluster-autoscaler/enabled = "true"`: Autoscaler discovers this node group
- `k8s.io/cluster-autoscaler/{cluster} = "owned"`: Autoscaler can manage it

#### EBS CSI Driver (IRSA)
```hcl
resource "aws_iam_role" "ebs_csi" {
  count = var.enable_ebs_csi_driver ? 1 : 0
  name  = "${local.cluster_name}-ebs-csi-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Effect = "Allow"
      Principal = {
        Federated = aws_iam_openid_connect_provider.cluster.arn
      }
      Action = "sts:AssumeRoleWithWebIdentity"
      Condition = {
        StringEquals = {
          "${local.oidc_issuer}:sub" = "system:serviceaccount:kube-system:ebs-csi-controller-sa"
        }
      }
    }]
  })
}

resource "aws_eks_addon" "ebs_csi" {
  count                    = var.enable_ebs_csi_driver ? 1 : 0
  cluster_name             = aws_eks_cluster.main.name
  addon_name               = "aws-ebs-csi-driver"
  service_account_role_arn = aws_iam_role.ebs_csi[0].arn
}
```

**What is EBS CSI Driver?**
- Creates EBS volumes for PersistentVolumeClaims
- Required for stateful apps (databases, etc.)
- Uses IRSA - only this pod can create EBS volumes

---

## Part 3: Enable EKS in Dev Environment

### Step 3.1: Navigate to Dev Environment
```bash
cd ~/Desktop/Devops-Project/techitfactory-infra/environments/dev
```

### Step 3.2: Check main.tf

The EKS module should already be uncommented. Verify:

```bash
grep -A 5 "module \"eks\"" main.tf
# Should show module "eks" { (not commented)
```

### Step 3.3: Understanding Dev EKS Configuration

```hcl
module "eks" {
  source = "../../modules/eks"

  project_name = local.project_name
  environment  = local.environment

  vpc_id     = module.vpc.vpc_id              # ðŸ‘ˆ From VPC module
  subnet_ids = module.vpc.private_subnet_ids  # ðŸ‘ˆ Nodes in private subnets

  cluster_version = "1.31"

  # Node configuration - Dev settings
  node_desired_size   = 2                     # Start with 2 nodes
  node_min_size       = 1                     # Can scale down to 1
  node_max_size       = 4                     # Can scale up to 4
  node_instance_types = ["t3.medium"]         # 2 vCPU, 4GB RAM
  node_capacity_type  = "ON_DEMAND"           # Could use SPOT for cost savings
  node_disk_size      = 50                    # 50GB EBS

  # Add-ons
  enable_ebs_csi_driver     = true            # For PersistentVolumes
  enable_cluster_autoscaler = true            # IRSA role for autoscaler
  enable_alb_controller     = true            # IRSA role for ALB
}
```

---

## Part 4: Apply EKS

### Step 4.1: Initialize
```bash
terraform init -upgrade
```

### Step 4.2: Plan
```bash
terraform plan

# Should show ~20-25 new resources:
# - EKS Cluster
# - IAM Roles (cluster, node, ebs-csi, autoscaler, alb-controller)
# - Security Group
# - Node Group
# - EKS Add-ons (CoreDNS, kube-proxy, VPC-CNI, EBS-CSI)
# - OIDC Provider
```

### Step 4.3: Apply (Takes 15-20 minutes!)
```bash
terraform apply

# Type 'yes' when prompted
# â˜• Go get coffee - EKS takes 15-20 minutes
```

### Step 4.4: Configure kubectl
```bash
# Get the kubeconfig command from output
terraform output kubeconfig_command

# Run it:
aws eks update-kubeconfig --name techitfactory-dev --region ap-south-1
```

### Step 4.5: Verify Connection
```bash
# Check nodes
kubectl get nodes
# NAME                                            STATUS   ROLES    AGE   VERSION
# ip-10-0-10-xxx.ap-south-1.compute.internal     Ready    <none>   5m    v1.28.x
# ip-10-0-20-xxx.ap-south-1.compute.internal     Ready    <none>   5m    v1.28.x

# Check system pods
kubectl get pods -n kube-system
# Should show: coredns, kube-proxy, aws-node (VPC CNI), ebs-csi
```

---

## Part 5: Verification

### Step 5.1: Check Cluster Status
```bash
aws eks describe-cluster --name techitfactory-dev \
  --query "cluster.{Name:name,Status:status,Version:version}" \
  --output table
```

### Step 5.2: Check Add-ons
```bash
aws eks list-addons --cluster-name techitfactory-dev
# ["coredns", "kube-proxy", "vpc-cni", "aws-ebs-csi-driver"]
```

### Step 5.3: Check IRSA Roles Created
```bash
terraform output cluster_autoscaler_role_arn
terraform output alb_controller_role_arn

# These are ready for Helm charts to use!
```

### Step 5.4: Test Pod Scheduling
```bash
# Create a test pod
kubectl run test-nginx --image=nginx --restart=Never

# Check it's running
kubectl get pod test-nginx
# NAME         READY   STATUS    RESTARTS   AGE
# test-nginx   1/1     Running   0          30s

# Clean up
kubectl delete pod test-nginx
```

---

## Cost Summary

| Resource | Monthly Cost | Notes |
|----------|--------------|-------|
| EKS Control Plane | ~$72 | Fixed cost |
| 2x t3.medium nodes | ~$60 | $0.0416/hour Ã— 2 Ã— 720 hours |
| EBS (50GB Ã— 2) | ~$10 | $0.10/GB-month |
| CloudWatch Logs | ~$5 | Control plane logs |
| **Total** | **~$147/month** | - |

---

## Commit and Push

```bash
cd ~/Desktop/Devops-Project/techitfactory-infra
git add .
git commit -m "Deploy EKS cluster with managed node group"
git push origin main
```

---

## Key Takeaways

1. **IRSA is essential**: Pods assume IAM roles without access keys
2. **Private subnets for nodes**: Nodes in private subnets, ALB in public
3. **Managed Node Groups**: AWS handles node updates and scaling
4. **Cluster Autoscaler tags**: Required for autoscaler to discover nodes
5. **EBS CSI Driver**: Required for PersistentVolumes in EKS 1.23+
6. **SSM access**: Use Session Manager instead of SSH keys

---

## Troubleshooting

### Nodes not joining
```bash
aws eks describe-nodegroup --cluster-name techitfactory-dev \
  --nodegroup-name techitfactory-dev-nodes \
  --query "nodegroup.{Status:status,Health:health}"
```

### kubectl connection issues
```bash
# Re-run kubeconfig update
aws eks update-kubeconfig --name techitfactory-dev --region ap-south-1

# Check current context
kubectl config current-context
```

### Pods stuck in Pending
```bash
# Check events
kubectl describe pod <pod-name>

# Common causes: no nodes, resource limits, node selector mismatch
```

---

## Next: Story 4.2 (Configure EKS Access)
