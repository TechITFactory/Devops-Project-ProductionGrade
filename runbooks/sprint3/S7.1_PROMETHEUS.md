# Sprint 4, Story 8.1: Install Prometheus Stack

## Story Summary
Install the Prometheus monitoring stack for metrics collection and alerting.

**What We're Installing:**
- Prometheus (metrics collection)
- Alertmanager (alert routing)
- Node Exporter (node metrics)
- kube-state-metrics (Kubernetes metrics)

---

## Prerequisites
- EKS cluster running
- kubectl configured
- Helm installed

---

## Part 1: Install kube-prometheus-stack

### Step 1.1: Add Helm Repository
```bash
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
```

### Step 1.2: Create Namespace
```bash
kubectl create namespace monitoring
```

### Step 1.3: Create Values File
```bash
cat > prometheus-values.yaml << 'EOF'
# Prometheus Stack Configuration

# Grafana
grafana:
  enabled: true
  adminPassword: "TechITFactory123!"  # Change this!
  persistence:
    enabled: true
    size: 10Gi
  ingress:
    enabled: false  # We'll create ALB ingress separately

# Prometheus
prometheus:
  prometheusSpec:
    retention: 15d
    storageSpec:
      volumeClaimTemplate:
        spec:
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 50Gi
    # Scrape configs for our apps
    additionalScrapeConfigs:
      - job_name: 'techitfactory-apps'
        kubernetes_sd_configs:
          - role: pod
            namespaces:
              names:
                - techitfactory
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__

# Alertmanager
alertmanager:
  alertmanagerSpec:
    storage:
      volumeClaimTemplate:
        spec:
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 10Gi

# Node Exporter - enabled by default
nodeExporter:
  enabled: true

# kube-state-metrics - enabled by default
kubeStateMetrics:
  enabled: true
EOF
```

### Step 1.4: Install Stack
```bash
helm install prometheus prometheus-community/kube-prometheus-stack \
  -n monitoring \
  -f prometheus-values.yaml \
  --wait
```

---

## Part 2: Access Prometheus UI

### Option A: Port Forward
```bash
kubectl port-forward svc/prometheus-kube-prometheus-prometheus -n monitoring 9090:9090
# Open http://localhost:9090
```

### Option B: ALB Ingress
```bash
kubectl apply -f - <<EOF
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: prometheus
  namespace: monitoring
  annotations:
    alb.ingress.kubernetes.io/scheme: internal  # Internal only!
    alb.ingress.kubernetes.io/target-type: ip
spec:
  ingressClassName: alb
  rules:
  - host: prometheus.internal.techitfactory.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: prometheus-kube-prometheus-prometheus
            port:
              number: 9090
EOF
```

---

## Part 3: Verify Metrics Collection

### Check Targets
```bash
# Via port-forward
curl http://localhost:9090/api/v1/targets | jq '.data.activeTargets | length'
```

### In Prometheus UI
1. Go to Status â†’ Targets
2. Verify all targets are "UP"
3. Check for:
   - kubernetes-apiservers
   - kubernetes-nodes
   - kubernetes-pods
   - kube-state-metrics

### Run Sample Query
```promql
# Node CPU usage
100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)

# Pod memory usage
container_memory_usage_bytes{namespace="techitfactory"}

# HTTP request rate
rate(http_requests_total[5m])
```

---

## Part 4: Configure Alerts

### Step 4.1: Create AlertManager Config
```bash
kubectl apply -f - <<EOF
apiVersion: v1
kind: Secret
metadata:
  name: alertmanager-prometheus-kube-prometheus-alertmanager
  namespace: monitoring
stringData:
  alertmanager.yaml: |
    global:
      resolve_timeout: 5m
    route:
      group_by: ['alertname', 'severity']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 1h
      receiver: 'slack-notifications'
      routes:
        - match:
            severity: critical
          receiver: 'slack-critical'
    receivers:
      - name: 'slack-notifications'
        slack_configs:
          - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
            channel: '#alerts'
            send_resolved: true
      - name: 'slack-critical'
        slack_configs:
          - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
            channel: '#alerts-critical'
            send_resolved: true
EOF
```

### Step 4.2: Create Custom Alert Rules
```bash
kubectl apply -f - <<EOF
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: techitfactory-rules
  namespace: monitoring
  labels:
    release: prometheus
spec:
  groups:
    - name: techitfactory
      rules:
        - alert: HighPodCPU
          expr: |
            (sum(rate(container_cpu_usage_seconds_total{namespace="techitfactory"}[5m])) 
            by (pod) / sum(kube_pod_container_resource_requests{namespace="techitfactory", resource="cpu"}) 
            by (pod)) > 0.8
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "High CPU usage detected"
            description: "Pod {{ \$labels.pod }} CPU usage is above 80%"
        
        - alert: PodNotReady
          expr: kube_pod_status_ready{namespace="techitfactory", condition="true"} == 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Pod not ready"
            description: "Pod {{ \$labels.pod }} has been not ready for 5 minutes"
        
        - alert: HighMemoryUsage
          expr: |
            (container_memory_usage_bytes{namespace="techitfactory"} 
            / container_spec_memory_limit_bytes{namespace="techitfactory"}) > 0.8
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "High memory usage"
            description: "Container {{ \$labels.container }} memory usage is above 80%"
EOF
```

---

## Part 5: Add Metrics to Applications

### Annotate Pods for Scraping
```yaml
# Add to deployment spec
metadata:
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "3000"
    prometheus.io/path: "/metrics"
```

### Node.js Example (with prom-client)
```javascript
const client = require('prom-client');
const collectDefaultMetrics = client.collectDefaultMetrics;
collectDefaultMetrics({ timeout: 5000 });

// Custom counter
const httpRequestsTotal = new client.Counter({
  name: 'http_requests_total',
  help: 'Total HTTP requests',
  labelNames: ['method', 'path', 'status']
});

app.get('/metrics', async (req, res) => {
  res.set('Content-Type', client.register.contentType);
  res.end(await client.register.metrics());
});
```

---

## Verification

### Check All Components
```bash
kubectl get pods -n monitoring
kubectl get svc -n monitoring
kubectl get prometheusrules -n monitoring
```

### Check Prometheus is Scraping
```bash
kubectl exec -n monitoring prometheus-prometheus-kube-prometheus-prometheus-0 -- \
  wget -qO- http://localhost:9090/api/v1/targets | head -50
```

---

## Story Completion Checklist
- [ ] Prometheus stack installed
- [ ] Prometheus UI accessible
- [ ] All targets showing "UP"
- [ ] Queries returning data
- [ ] AlertManager configured
- [ ] Custom alert rules created
- [ ] Applications annotated for scraping

---

## Next: Story 8.2 (Grafana Dashboards)
