# Sprint 2, Story 4.1: Provision EKS Cluster

## Story Summary
Implement the EKS module to create a production-ready Kubernetes cluster.

**What We're Building:**
- EKS Control Plane with OIDC
- Managed Node Group (t3.medium)
- IAM Roles for cluster and nodes
- EBS CSI Driver for PersistentVolumes
- OIDC Provider for IRSA

---

## Prerequisites
- Epic 3 completed (VPC deployed)
- VPC outputs available (vpc_id, private_subnet_ids)
- AWS credentials configured

---

## Part 1: Implement EKS Module

### Step 1.1: Navigate to EKS Module
```bash
cd ~/Desktop/Devops-Project/techitfactory-infra/modules/eks
```

### Step 1.2: Update main.tf with Implementation
```bash
cat > main.tf << 'EOF'
# =============================================================================
# EKS MODULE - IMPLEMENTATION
# =============================================================================

locals {
  cluster_name = "${var.project_name}-${var.environment}"

  common_tags = merge(
    {
      Module      = "eks"
      Environment = var.environment
    },
    var.tags
  )
}

data "aws_region" "current" {}
data "aws_caller_identity" "current" {}

# =============================================================================
# IAM ROLE FOR EKS CLUSTER
# =============================================================================

resource "aws_iam_role" "cluster" {
  name = "${local.cluster_name}-cluster-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Action = "sts:AssumeRole"
      Effect = "Allow"
      Principal = {
        Service = "eks.amazonaws.com"
      }
    }]
  })

  tags = merge(local.common_tags, {
    Name = "${local.cluster_name}-cluster-role"
  })
}

resource "aws_iam_role_policy_attachment" "cluster_policy" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSClusterPolicy"
  role       = aws_iam_role.cluster.name
}

resource "aws_iam_role_policy_attachment" "cluster_vpc_controller" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSVPCResourceController"
  role       = aws_iam_role.cluster.name
}

# =============================================================================
# CLUSTER SECURITY GROUP
# =============================================================================

resource "aws_security_group" "cluster" {
  name        = "${local.cluster_name}-cluster-sg"
  description = "Security group for EKS cluster"
  vpc_id      = var.vpc_id

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = merge(local.common_tags, {
    Name = "${local.cluster_name}-cluster-sg"
  })
}

# =============================================================================
# EKS CLUSTER
# =============================================================================

resource "aws_eks_cluster" "main" {
  name     = local.cluster_name
  role_arn = aws_iam_role.cluster.arn
  version  = var.cluster_version

  vpc_config {
    subnet_ids              = var.subnet_ids
    endpoint_public_access  = var.cluster_endpoint_public_access
    endpoint_private_access = var.cluster_endpoint_private_access
    security_group_ids      = [aws_security_group.cluster.id]
  }

  # Enable control plane logging
  enabled_cluster_log_types = ["api", "audit", "authenticator", "controllerManager", "scheduler"]

  tags = merge(local.common_tags, {
    Name = local.cluster_name
  })

  depends_on = [
    aws_iam_role_policy_attachment.cluster_policy,
    aws_iam_role_policy_attachment.cluster_vpc_controller,
  ]
}

# =============================================================================
# OIDC PROVIDER FOR IRSA
# =============================================================================

data "tls_certificate" "cluster" {
  url = aws_eks_cluster.main.identity[0].oidc[0].issuer
}

resource "aws_iam_openid_connect_provider" "cluster" {
  url             = aws_eks_cluster.main.identity[0].oidc[0].issuer
  client_id_list  = ["sts.amazonaws.com"]
  thumbprint_list = [data.tls_certificate.cluster.certificates[0].sha1_fingerprint]

  tags = merge(local.common_tags, {
    Name = "${local.cluster_name}-oidc"
  })
}

# =============================================================================
# IAM ROLE FOR NODE GROUP
# =============================================================================

resource "aws_iam_role" "node" {
  name = "${local.cluster_name}-node-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Action = "sts:AssumeRole"
      Effect = "Allow"
      Principal = {
        Service = "ec2.amazonaws.com"
      }
    }]
  })

  tags = merge(local.common_tags, {
    Name = "${local.cluster_name}-node-role"
  })
}

resource "aws_iam_role_policy_attachment" "node_worker" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"
  role       = aws_iam_role.node.name
}

resource "aws_iam_role_policy_attachment" "node_cni" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"
  role       = aws_iam_role.node.name
}

resource "aws_iam_role_policy_attachment" "node_ecr" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
  role       = aws_iam_role.node.name
}

# SSM for Session Manager access to nodes
resource "aws_iam_role_policy_attachment" "node_ssm" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore"
  role       = aws_iam_role.node.name
}

# =============================================================================
# MANAGED NODE GROUP
# =============================================================================

resource "aws_eks_node_group" "main" {
  cluster_name    = aws_eks_cluster.main.name
  node_group_name = "${local.cluster_name}-nodes"
  node_role_arn   = aws_iam_role.node.arn
  subnet_ids      = var.subnet_ids

  instance_types = var.node_instance_types
  capacity_type  = var.node_capacity_type
  disk_size      = var.node_disk_size

  scaling_config {
    desired_size = var.node_desired_size
    max_size     = var.node_max_size
    min_size     = var.node_min_size
  }

  update_config {
    max_unavailable = 1
  }

  labels = {
    role        = "general"
    environment = var.environment
  }

  tags = merge(local.common_tags, {
    Name                                        = "${local.cluster_name}-nodes"
    "k8s.io/cluster-autoscaler/enabled"         = "true"
    "k8s.io/cluster-autoscaler/${local.cluster_name}" = "owned"
  })

  depends_on = [
    aws_iam_role_policy_attachment.node_worker,
    aws_iam_role_policy_attachment.node_cni,
    aws_iam_role_policy_attachment.node_ecr,
  ]
}

# =============================================================================
# EBS CSI DRIVER IRSA ROLE
# =============================================================================

locals {
  oidc_issuer = replace(aws_eks_cluster.main.identity[0].oidc[0].issuer, "https://", "")
}

resource "aws_iam_role" "ebs_csi" {
  count = var.enable_ebs_csi_driver ? 1 : 0
  name  = "${local.cluster_name}-ebs-csi-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Effect = "Allow"
      Principal = {
        Federated = aws_iam_openid_connect_provider.cluster.arn
      }
      Action = "sts:AssumeRoleWithWebIdentity"
      Condition = {
        StringEquals = {
          "${local.oidc_issuer}:aud" = "sts.amazonaws.com"
          "${local.oidc_issuer}:sub" = "system:serviceaccount:kube-system:ebs-csi-controller-sa"
        }
      }
    }]
  })

  tags = merge(local.common_tags, {
    Name = "${local.cluster_name}-ebs-csi-role"
  })
}

resource "aws_iam_role_policy_attachment" "ebs_csi" {
  count      = var.enable_ebs_csi_driver ? 1 : 0
  policy_arn = "arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy"
  role       = aws_iam_role.ebs_csi[0].name
}

# EBS CSI Driver Add-on
resource "aws_eks_addon" "ebs_csi" {
  count                    = var.enable_ebs_csi_driver ? 1 : 0
  cluster_name             = aws_eks_cluster.main.name
  addon_name               = "aws-ebs-csi-driver"
  service_account_role_arn = aws_iam_role.ebs_csi[0].arn
  resolve_conflicts_on_create = "OVERWRITE"

  depends_on = [aws_eks_node_group.main]
}

# =============================================================================
# COREDNS & KUBE-PROXY ADD-ONS
# =============================================================================

resource "aws_eks_addon" "coredns" {
  cluster_name = aws_eks_cluster.main.name
  addon_name   = "coredns"
  resolve_conflicts_on_create = "OVERWRITE"

  depends_on = [aws_eks_node_group.main]
}

resource "aws_eks_addon" "kube_proxy" {
  cluster_name = aws_eks_cluster.main.name
  addon_name   = "kube-proxy"
  resolve_conflicts_on_create = "OVERWRITE"
}

resource "aws_eks_addon" "vpc_cni" {
  cluster_name = aws_eks_cluster.main.name
  addon_name   = "vpc-cni"
  resolve_conflicts_on_create = "OVERWRITE"
}
EOF
```

### Step 1.3: Update outputs.tf
```bash
cat > outputs.tf << 'EOF'
# =============================================================================
# EKS MODULE - OUTPUTS
# =============================================================================

output "cluster_id" {
  description = "EKS cluster ID"
  value       = aws_eks_cluster.main.id
}

output "cluster_name" {
  description = "EKS cluster name"
  value       = aws_eks_cluster.main.name
}

output "cluster_endpoint" {
  description = "EKS cluster API endpoint"
  value       = aws_eks_cluster.main.endpoint
}

output "cluster_certificate_authority_data" {
  description = "Base64 encoded certificate for cluster CA"
  value       = aws_eks_cluster.main.certificate_authority[0].data
}

output "cluster_version" {
  description = "Kubernetes version"
  value       = aws_eks_cluster.main.version
}

output "cluster_oidc_issuer_url" {
  description = "OIDC issuer URL (for IRSA role trust policies)"
  value       = aws_eks_cluster.main.identity[0].oidc[0].issuer
}

output "oidc_provider_arn" {
  description = "OIDC provider ARN"
  value       = aws_iam_openid_connect_provider.cluster.arn
}

output "cluster_security_group_id" {
  description = "Security group ID for cluster"
  value       = aws_security_group.cluster.id
}

output "node_group_name" {
  description = "Name of the node group"
  value       = aws_eks_node_group.main.node_group_name
}

output "node_role_arn" {
  description = "IAM role ARN for nodes"
  value       = aws_iam_role.node.arn
}

output "kubeconfig_command" {
  description = "Command to update kubeconfig"
  value       = "aws eks update-kubeconfig --name ${aws_eks_cluster.main.name} --region ${data.aws_region.current.name}"
}
EOF
```

---

## Part 2: Update Dev Environment

### Step 2.1: Enable EKS Module in Dev
```bash
cd ~/Desktop/Devops-Project/techitfactory-infra/environments/dev

# Edit main.tf to uncomment the EKS module block
```

---

## Part 3: Apply EKS

### Step 3.1: Initialize and Plan
```bash
cd ~/Desktop/Devops-Project/techitfactory-infra/environments/dev
terraform init -upgrade
terraform plan
# Should show ~15-20 resources to create
```

### Step 3.2: Apply (takes 10-15 minutes)
```bash
terraform apply
# Type 'yes' when prompted
# EKS creation takes ~10-15 minutes
```

### Step 3.3: Configure kubectl
```bash
# Get the kubeconfig command from output
terraform output kubeconfig_command

# Run it
aws eks update-kubeconfig --name techitfactory-dev --region ap-south-1

# Verify
kubectl get nodes
kubectl get pods -n kube-system
```

---

## Verification

### Check Cluster Status
```bash
aws eks describe-cluster --name techitfactory-dev \
  --query "cluster.{Name:name,Status:status,Version:version,Endpoint:endpoint}"
```

### Check Nodes
```bash
kubectl get nodes -o wide
# Should show 2 nodes (or your desired count)
```

### Check Add-ons
```bash
aws eks list-addons --cluster-name techitfactory-dev
# Should show: vpc-cni, coredns, kube-proxy, aws-ebs-csi-driver
```

### Check OIDC Provider
```bash
aws iam list-open-id-connect-providers | grep techitfactory-dev
```

---

## Cost Summary

| Resource | Monthly Cost |
|----------|--------------|
| EKS Control Plane | ~$72 |
| 2x t3.medium nodes | ~$60 |
| EBS (50GB Ã— 2) | ~$10 |
| CloudWatch Logs | ~$5 |
| **Total** | **~$147/month** |

---

## Commit and Push

```bash
cd ~/Desktop/Devops-Project/techitfactory-infra
git add modules/eks/
git add environments/dev/
git commit -m "Implement EKS module with managed node group"
git push origin main
```

---

## Story Completion Checklist
- [ ] modules/eks/main.tf implemented
- [ ] modules/eks/outputs.tf updated
- [ ] EKS module enabled in dev environment
- [ ] terraform apply successful
- [ ] EKS cluster running
- [ ] Node group healthy
- [ ] kubectl configured and working
- [ ] Add-ons installed (CoreDNS, kube-proxy, VPC CNI, EBS CSI)
- [ ] OIDC provider created
- [ ] Changes committed and pushed

---

## Next: Story 4.2 (Configure SSO Access)
