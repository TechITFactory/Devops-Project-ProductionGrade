# Sprint 2, Story 4.3: Install Cluster Autoscaler

## Story Summary
Deploy Cluster Autoscaler to automatically scale node groups based on pod demand.

---

## Why Are We Doing This?

**The Problem:**
You deploy an app with 10 replicas, but your 2 nodes can only fit 6 pods. The remaining 4 pods stay in "Pending" state forever. You have to manually add nodes via AWS Console or Terraform.

**The Solution:**
Cluster Autoscaler watches for pending pods and automatically adds nodes. When pods are deleted and nodes become underutilized, it removes nodes to save money.

**Real-world scenario:**
```
8:00 AM  - Traffic low     → 2 nodes running
10:00 AM - Traffic spike   → 4 nodes running (autoscaled up)
2:00 PM  - Traffic normal  → 3 nodes running
2:00 AM  - Traffic minimal → 1 node running (autoscaled down)
```

**Cost savings:** Instead of paying for 4 nodes 24/7, you only pay for what you need.

---

## Runbook Overview

| Part | What We're Doing | Why |
|------|------------------|-----|
| 0 | Verify IRSA role | Autoscaler needs AWS permissions |
| 1 | Understand flow | Know what autoscaler does |
| 2 | Deploy via Helm | Install autoscaler pod |
| 3 | Test scale-up | Prove it works |
| 4 | Verify | Check logs/status |

---

## Part 0: Verify Prerequisites

### Step 0.1: Check IRSA Role Exists

**Why?** Cluster Autoscaler needs to call AWS APIs to:
- Describe Auto Scaling Groups (ASGs)
- Increase/decrease ASG desired capacity
- Terminate instances

The EKS module already created this role with the right permissions.

```bash
cd ~/Desktop/Devops-Project/techitfactory-infra/environments/dev

terraform output cluster_autoscaler_role_arn
# Output: arn:aws:iam::ACCOUNT:role/techitfactory-dev-cluster-autoscaler
```

### Step 0.2: Check Node Group Tags

**Why?** Autoscaler uses tags to discover which node groups to manage. Without these tags, autoscaler won't know your nodes exist!

```bash
aws eks describe-nodegroup \
  --cluster-name techitfactory-dev \
  --nodegroup-name techitfactory-dev-nodes \
  --query "nodegroup.tags"
```

**Required tags:**
| Tag | Value | Why |
|-----|-------|-----|
| `k8s.io/cluster-autoscaler/enabled` | `true` | "I can be scaled" |
| `k8s.io/cluster-autoscaler/techitfactory-dev` | `owned` | "Autoscaler owns me" |

---

## Part 1: Understanding Cluster Autoscaler

### How It Works

**Scale Up:**
1. You deploy pods that request more CPU/memory than available
2. Pods show "Pending" status (no node can fit them)
3. Autoscaler sees pending pods
4. Autoscaler calls AWS: "Increase ASG from 2 to 3 nodes"
5. New node joins cluster
6. Pending pods get scheduled on new node

**Scale Down:**
1. You delete your deployment
2. Node becomes underutilized (< 50% used)
3. After 10 minutes, autoscaler says "This node is wasting money"
4. Autoscaler moves pods to other nodes
5. Autoscaler calls AWS: "Terminate this instance"
6. Node removed, money saved

### Diagram
```
Pod Pending? ──▶ Yes ──▶ Can a new node fit it? ──▶ Yes ──▶ Scale Up
     │                          │
     ▼                          ▼
    No                         No
     │                          │
     ▼                          ▼
  Nothing                   Pod stays pending
  (nodes ok)                (hit max nodes)
```

---

## Part 2: Deploy Cluster Autoscaler via Helm

### Step 2.1: Add Helm Repository

**Why Helm?** Helm is like apt/yum for Kubernetes. Instead of writing 200 lines of YAML, we run one command.

```bash
helm repo add autoscaler https://kubernetes.github.io/autoscaler
helm repo update
```

### Step 2.2: Get IRSA Role ARN

**Why?** We need to tell the autoscaler pod "Use this IAM role for AWS API calls"

```bash
cd ~/Desktop/Devops-Project/techitfactory-infra/environments/dev

AUTOSCALER_ROLE_ARN=$(terraform output -raw cluster_autoscaler_role_arn)
echo "Using role: $AUTOSCALER_ROLE_ARN"
```

### Step 2.3: Install Cluster Autoscaler

```bash
helm install cluster-autoscaler autoscaler/cluster-autoscaler \
  --namespace kube-system \
  --set autoDiscovery.clusterName=techitfactory-dev \
  --set awsRegion=ap-south-1 \
  --set rbac.serviceAccount.create=true \
  --set rbac.serviceAccount.name=cluster-autoscaler \
  --set rbac.serviceAccount.annotations."eks\.amazonaws\.com/role-arn"=$AUTOSCALER_ROLE_ARN \
  --set extraArgs.balance-similar-node-groups=true \
  --set extraArgs.skip-nodes-with-system-pods=false
```

**What each flag does:**
| Flag | Why |
|------|-----|
| `autoDiscovery.clusterName` | Find node groups by cluster name tag |
| `awsRegion` | Which region to call AWS APIs |
| `serviceAccount.annotations...` | IRSA magic - pod assumes IAM role |
| `balance-similar-node-groups` | Spread pods across AZs evenly |
| `skip-nodes-with-system-pods=false` | Can scale down nodes with system pods |

### Step 2.4: Verify Installation
```bash
# Check pod is running
kubectl get pods -n kube-system | grep autoscaler

# Check logs for success
kubectl logs deployment/cluster-autoscaler -n kube-system | head -20
# Look for: "Successfully fetched nodegroups"
```

---

## Part 3: Test Autoscaling (Demo)

**Goal:** Prove autoscaler adds nodes when we need more capacity.

### Step 3.1: Check Current State
```bash
kubectl get nodes
# You should see 2 nodes

kubectl get pods -A | wc -l
# Count existing pods
```

### Step 3.2: Create Pods That Don't Fit

**What we're doing:** Creating 10 pods that each need 500m CPU (0.5 cores). Since each t3.medium has ~2 cores, only ~4 pods fit per node. With 2 nodes = 8 pods max. We're creating 10, so 2 will be pending.

```bash
cat << 'EOF' | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: inflate
spec:
  replicas: 10
  selector:
    matchLabels:
      app: inflate
  template:
    metadata:
      labels:
        app: inflate
    spec:
      containers:
      - name: inflate
        image: public.ecr.aws/eks-distro/kubernetes/pause:3.7
        resources:
          requests:
            cpu: 500m
            memory: 256Mi
EOF
```

### Step 3.3: Watch the Magic

**Terminal 1 - Watch pods:**
```bash
kubectl get pods -w
# Some pods will show "Pending"
```

**Terminal 2 - Watch nodes:**
```bash
watch kubectl get nodes
# After 1-2 minutes, new node appears!
```

**Terminal 3 - Watch autoscaler logs:**
```bash
kubectl logs -f deployment/cluster-autoscaler -n kube-system
# Look for: "Expanding Node Group"
```

### Step 3.4: Cleanup
```bash
kubectl delete deployment inflate
# Nodes will scale down after ~10 minutes (scale-down-delay)
```

---

## Part 4: Verification

### Check Everything is Working
```bash
# Autoscaler pod running
kubectl get deployment cluster-autoscaler -n kube-system

# No errors in logs
kubectl logs deployment/cluster-autoscaler -n kube-system --tail=20 | grep -i error

# Node group config
aws eks describe-nodegroup \
  --cluster-name techitfactory-dev \
  --nodegroup-name techitfactory-dev-nodes \
  --query "nodegroup.scalingConfig"
# Should show: min=1, max=4, desired=2
```

---

## Troubleshooting

### "Pods still pending, no scale up"
```bash
# Check autoscaler can see node groups
kubectl logs deployment/cluster-autoscaler -n kube-system | grep nodegroup

# Check IRSA is working
kubectl describe sa cluster-autoscaler -n kube-system | grep Annotations
# Should show: eks.amazonaws.com/role-arn: ...
```

### "Hit max nodes but still need more"
Edit node group max size:
```bash
aws eks update-nodegroup-config \
  --cluster-name techitfactory-dev \
  --nodegroup-name techitfactory-dev-nodes \
  --scaling-config minSize=1,maxSize=10,desiredSize=2
```

---

## Key Takeaways

1. **Autoscaler needs AWS permissions** → IRSA role
2. **Node groups need tags** → for autodiscovery
3. **Scale up is fast** → 1-2 minutes
4. **Scale down is slow** → 10+ minutes (protects against thrashing)
5. **Saves money** → Pay only for what you use

---

## Next: Story 4.4 (Baseline Add-ons)
